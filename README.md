# TensorFlow-Step-by-step
a comparison of sklean and tensorflow
#### 线性回归： 
对于tensorflow，梯度下降的步长alpha参数需要很仔细的设置，步子太大容易扯到蛋导致无法收敛；步子太小容易等得蛋疼。迭代次数也需要细致的尝试。 
#### 多元线性回归： 
对于梯度下降算法，变量是否表转化很重要。在这个例子中，变量一个是面积，一个是房间数，量级相差很大，如果不归一化，面积在目标函数和梯度中就会占据主导地位，导致收敛极慢
#### 逻辑回归：
* 对于逻辑回归，损失函数比线性回归模型复杂了一些。首先需要通过sigmoid函数，将线性回归的结果转化为0至1之间的概率。然后写出每个样本的发生概率（似然）， 那么所有样本的发生概率就是每个样本发生概率的乘积。为了求导方便，我们对所有样本的发生概率取对数，保持其单调性的同时，可以将连乘变成求和，对数极大似然估计方法的目标函数就是最大化所有样本的发生概率；机器学习的习惯将目标函数称为损失， 所以将损失定义为对数似然的相反数， 以转化为极小值问题。 

* 我们提到逻辑回归时，一般是指而分类问题；然后这套思想的是可以轻松就拓展为多分类问题的，在机器学习领域一般称之为softmax回归模型。

#### 基于MNIST数据的softmax regression 
* sklearn的估计时间有点长， 因为每一轮参数更新都是基于全量的训练集数据算出损失，再算出梯度，然后再改进结果的。  
* tensorflow采用batch Gradient Descent估计算法时，时间也比较长，原因同上。  
* tensorflow采用stochastic Gradient Descent估计算法时间短，最后的估计结果也挺好，相当于每轮迭代只用到了部分数据集算出损失和梯度，速度变快，但可能bias增加；所以把迭代次数增多，这样可以降低variance， 总体上误差相比Batch Gradient Descent 并没有差多少。  



